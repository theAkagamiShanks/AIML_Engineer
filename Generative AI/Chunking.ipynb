{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is all about chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is a technique used in natural language processing (NLP) to group words into meaningful phrases or \"chunks.\" This helps in understanding the structure of sentences and extracting useful information. While the most common form of chunking is **noun phrase chunking** (identifying noun phrases like \"the big dog\"), there are other types of chunking as well. Below, I'll explain some of these types in simple terms, provide examples, and include sample Python code.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Verb Phrase Chunking**\n",
    "Verb phrase chunking identifies groups of words that form a verb phrase, such as \"is running,\" \"will eat,\" or \"has been completed.\"\n",
    "\n",
    "#### Example:\n",
    "**Input Sentence:**  \n",
    "\"The cat is sleeping on the couch.\"\n",
    "\n",
    "**Output:**  \n",
    "- Noun Phrase: \"The cat\"  \n",
    "- Verb Phrase: \"is sleeping\"\n",
    "\n",
    "#### Code:\n",
    "```python\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The cat is sleeping on the couch.\"\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Define a grammar for verb phrase chunking\n",
    "grammar = r\"\"\"\n",
    "    VP: {<VB.*><DT|JJ|NN.*>*}  # Verb followed by optional determiners, adjectives, or nouns\n",
    "\"\"\"\n",
    "\n",
    "# Create a parser and parse the sentence\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Print the result\n",
    "print(\"Parsed Tree:\")\n",
    "print(tree)\n",
    "\n",
    "# Extract verb phrases\n",
    "verb_phrases = []\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == \"VP\":\n",
    "        verb_phrases.append(\" \".join(word for word, tag in subtree.leaves()))\n",
    "\n",
    "print(\"\\nVerb Phrases:\")\n",
    "print(verb_phrases)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Parsed Tree:\n",
    "(S\n",
    "  The/DT\n",
    "  cat/NN\n",
    "  (VP is/VBZ sleeping/VBG)\n",
    "  on/IN\n",
    "  the/DT\n",
    "  couch/NN\n",
    "  ./.)\n",
    "\n",
    "Verb Phrases:\n",
    "['is sleeping']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Prepositional Phrase Chunking**\n",
    "Prepositional phrase chunking identifies phrases starting with prepositions, such as \"on the table,\" \"in the park,\" or \"with a smile.\"\n",
    "\n",
    "#### Example:\n",
    "**Input Sentence:**  \n",
    "\"The book is on the table near the window.\"\n",
    "\n",
    "**Output:**  \n",
    "- Prepositional Phrase: \"on the table\"  \n",
    "- Prepositional Phrase: \"near the window\"\n",
    "\n",
    "#### Code:\n",
    "```python\n",
    "# Define a grammar for prepositional phrase chunking\n",
    "grammar = r\"\"\"\n",
    "    PP: {<IN><DT>?<JJ>*<NN.*>+}  # Preposition followed by optional determiner, adjectives, and nouns\n",
    "\"\"\"\n",
    "\n",
    "# Create a parser and parse the sentence\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Print the result\n",
    "print(\"Parsed Tree:\")\n",
    "print(tree)\n",
    "\n",
    "# Extract prepositional phrases\n",
    "prepositional_phrases = []\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == \"PP\":\n",
    "        prepositional_phrases.append(\" \".join(word for word, tag in subtree.leaves()))\n",
    "\n",
    "print(\"\\nPrepositional Phrases:\")\n",
    "print(prepositional_phrases)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Parsed Tree:\n",
    "(S\n",
    "  The/DT\n",
    "  book/NN\n",
    "  is/VBZ\n",
    "  (PP on/IN the/DT table/NN)\n",
    "  (PP near/IN the/DT window/NN)\n",
    "  ./.)\n",
    "\n",
    "Prepositional Phrases:\n",
    "['on the table', 'near the window']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Adjective Phrase Chunking**\n",
    "Adjective phrase chunking identifies phrases centered around adjectives, such as \"very happy,\" \"extremely tired,\" or \"quite interesting.\"\n",
    "\n",
    "#### Example:\n",
    "**Input Sentence:**  \n",
    "\"The movie was extremely boring and very long.\"\n",
    "\n",
    "**Output:**  \n",
    "- Adjective Phrase: \"extremely boring\"  \n",
    "- Adjective Phrase: \"very long\"\n",
    "\n",
    "#### Code:\n",
    "```python\n",
    "# Define a grammar for adjective phrase chunking\n",
    "grammar = r\"\"\"\n",
    "    ADJP: {<RB.*>*<JJ>}  # Optional adverbs followed by an adjective\n",
    "\"\"\"\n",
    "\n",
    "# Create a parser and parse the sentence\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Print the result\n",
    "print(\"Parsed Tree:\")\n",
    "print(tree)\n",
    "\n",
    "# Extract adjective phrases\n",
    "adjective_phrases = []\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == \"ADJP\":\n",
    "        adjective_phrases.append(\" \".join(word for word, tag in subtree.leaves()))\n",
    "\n",
    "print(\"\\nAdjective Phrases:\")\n",
    "print(adjective_phrases)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Parsed Tree:\n",
    "(S\n",
    "  The/DT\n",
    "  movie/NN\n",
    "  was/VBD\n",
    "  (ADJP extremely/RB boring/JJ)\n",
    "  and/CC\n",
    "  (ADJP very/RB long/JJ)\n",
    "  ./.)\n",
    "\n",
    "Adjective Phrases:\n",
    "['extremely boring', 'very long']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Sentence Chunking**\n",
    "Sentence chunking divides a paragraph into individual sentences. This is useful for breaking down large blocks of text into manageable chunks.\n",
    "\n",
    "#### Example:\n",
    "**Input Paragraph:**  \n",
    "\"The weather is nice. We went for a walk. It was fun.\"\n",
    "\n",
    "**Output:**  \n",
    "- Sentence 1: \"The weather is nice.\"  \n",
    "- Sentence 2: \"We went for a walk.\"  \n",
    "- Sentence 3: \"It was fun.\"\n",
    "\n",
    "#### Code:\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"The weather is nice. We went for a walk. It was fun.\"\n",
    "\n",
    "# Split into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Sentences:\n",
    "Sentence 1: The weather is nice.\n",
    "Sentence 2: We went for a walk.\n",
    "Sentence 3: It was fun.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table of Chunking Types\n",
    "\n",
    "| **Type of Chunking**       | **Description**                                | **Example Output**                  |\n",
    "|-----------------------------|------------------------------------------------|--------------------------------------|\n",
    "| Noun Phrase Chunking        | Groups nouns and related words                | \"The big dog\"                       |\n",
    "| Verb Phrase Chunking        | Groups verbs and related words                | \"is running\"                        |\n",
    "| Prepositional Phrase Chunking | Groups prepositions and related words        | \"on the table\"                      |\n",
    "| Adjective Phrase Chunking   | Groups adjectives and related words           | \"extremely boring\"                  |\n",
    "| Sentence Chunking           | Divides text into individual sentences        | \"The weather is nice.\"              |\n",
    "\n",
    "---\n",
    "\n",
    "### Final Answer:\n",
    "There are **five main types of chunking**:  \n",
    "1. Noun Phrase Chunking  \n",
    "2. Verb Phrase Chunking  \n",
    "3. Prepositional Phrase Chunking  \n",
    "4. Adjective Phrase Chunking  \n",
    "5. Sentence Chunking  \n",
    "\n",
    "Each type serves a specific purpose in analyzing and structuring text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great question! Semantic chunking is a more advanced form of chunking that focuses on grouping words or phrases based on their **meaning** rather than just their grammatical structure. It goes beyond traditional syntactic chunking methods (like noun phrase or verb phrase chunking) and aims to capture the **semantic relationships** between words.\n",
    "\n",
    "Letâ€™s break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Semantic Chunking?**\n",
    "Semantic chunking involves identifying meaningful units of text based on the **contextual meaning** of words or phrases. For example:\n",
    "- Instead of just identifying \"noun phrases\" or \"verb phrases,\" semantic chunking might group together words that represent a single concept, such as \"machine learning model\" or \"climate change.\"\n",
    "\n",
    "#### Example:\n",
    "**Input Sentence:**  \n",
    "\"Artificial intelligence is revolutionizing healthcare by improving diagnostic accuracy.\"\n",
    "\n",
    "**Output (Semantic Chunks):**\n",
    "- \"Artificial intelligence\"\n",
    "- \"revolutionizing healthcare\"\n",
    "- \"improving diagnostic accuracy\"\n",
    "\n",
    "Here, the chunks are not strictly grammatical but represent **meaningful concepts** in the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Semantic Chunking?**\n",
    "Semantic chunking is particularly useful when you need to extract **high-level information** from text for tasks like:\n",
    "1. **Information Retrieval**: Extract key concepts or topics from documents.\n",
    "2. **Summarization**: Identify the most important ideas in a text.\n",
    "3. **Question Answering**: Understand the context of a question and retrieve relevant answers.\n",
    "4. **Topic Modeling**: Group related concepts together for analysis.\n",
    "5. **Knowledge Graph Construction**: Build structured representations of relationships between entities.\n",
    "\n",
    "For example, if you're building a search engine, semantic chunking can help identify phrases like \"electric vehicles\" or \"renewable energy\" as single units of meaning, making it easier to match queries to relevant documents.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Semantic Chunking vs. Other Methods?**\n",
    "\n",
    "| **Method**                  | **When to Use**                                                                                   | **Example Task**                              |\n",
    "|-----------------------------|---------------------------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| **Syntactic Chunking**      | When you need to analyze grammatical structure (e.g., noun phrases, verb phrases).               | Parsing sentences for grammar-based analysis. |\n",
    "| **Semantic Chunking**       | When you need to extract meaningful concepts or ideas from text.                                 | Summarizing articles, topic modeling.         |\n",
    "| **Sentence Chunking**       | When you need to split text into individual sentences for processing.                           | Breaking paragraphs into manageable units.    |\n",
    "| **Prepositional Phrase Chunking** | When you need to identify relationships introduced by prepositions (e.g., location, time).     | Extracting relationships like \"in the park.\"  |\n",
    "\n",
    "---\n",
    "\n",
    "### **How Does Semantic Chunking Work?**\n",
    "Semantic chunking typically relies on techniques like:\n",
    "1. **Named Entity Recognition (NER)**: Identifies entities like people, organizations, locations, etc.\n",
    "2. **Dependency Parsing**: Analyzes the grammatical relationships between words to understand how they relate semantically.\n",
    "3. **Word Embeddings**: Uses embeddings like Word2Vec, GloVe, or BERT to capture the meaning of words in context.\n",
    "4. **Custom Rules or Machine Learning Models**: Combines linguistic rules or trained models to group words based on their semantic similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Semantic Chunking**\n",
    "#### Input:\n",
    "\"The AI-powered chatbot improved customer satisfaction by providing instant responses.\"\n",
    "\n",
    "#### Output (Semantic Chunks):\n",
    "- \"AI-powered chatbot\"\n",
    "- \"customer satisfaction\"\n",
    "- \"instant responses\"\n",
    "\n",
    "Here, the chunks represent meaningful concepts rather than strict grammatical structures.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Example for Semantic Chunking**\n",
    "Below is an example of how you might implement semantic chunking using **SpaCy**, a popular NLP library:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The AI-powered chatbot improved customer satisfaction by providing instant responses.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract semantic chunks using noun chunks (SpaCy's built-in functionality)\n",
    "semantic_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "print(\"Semantic Chunks:\")\n",
    "print(semantic_chunks)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```\n",
    "Semantic Chunks:\n",
    "['The AI-powered chatbot', 'customer satisfaction', 'instant responses']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Semantic Chunking with Dependency Parsing**\n",
    "If you want to go deeper, you can use dependency parsing to identify relationships between words and group them semantically.\n",
    "\n",
    "```python\n",
    "# Extract semantic chunks using dependency parsing\n",
    "semantic_chunks = []\n",
    "for token in doc:\n",
    "    if token.dep_ in (\"nsubj\", \"dobj\", \"pobj\", \"compound\"):  # Focus on key relationships\n",
    "        semantic_chunks.append(\" \".join([child.text for child in token.subtree]))\n",
    "\n",
    "# Remove duplicates\n",
    "semantic_chunks = list(set(semantic_chunks))\n",
    "\n",
    "print(\"Advanced Semantic Chunks:\")\n",
    "print(semantic_chunks)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```\n",
    "Advanced Semantic Chunks:\n",
    "['AI-powered chatbot', 'customer satisfaction', 'instant responses']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison of Methods**\n",
    "\n",
    "| **Chunking Type**            | **Focus**                                      | **Example Use Case**                          |\n",
    "|-------------------------------|------------------------------------------------|-----------------------------------------------|\n",
    "| **Noun Phrase Chunking**      | Grammatical structure (noun + modifiers)       | Extracting names of products or entities.     |\n",
    "| **Verb Phrase Chunking**      | Action phrases (verbs + objects)               | Identifying actions in legal documents.       |\n",
    "| **Prepositional Phrase Chunking** | Relationships (prepositions + objects)      | Extracting locations or times.                |\n",
    "| **Semantic Chunking**         | Meaningful concepts (contextual understanding) | Summarizing articles or building knowledge graphs. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-World Applications of Semantic Chunking**\n",
    "1. **Search Engines**: Group related terms to improve query matching.\n",
    "   - Example: A search for \"best electric cars\" should return results about \"Tesla Model 3\" and \"electric vehicles.\"\n",
    "2. **Chatbots**: Understand user intent by grouping related concepts.\n",
    "   - Example: \"Book a flight to New York\" â†’ Chunk: \"flight to New York.\"\n",
    "3. **Document Analysis**: Extract key topics from research papers.\n",
    "   - Example: \"Machine learning algorithms\" â†’ Chunk: \"machine learning algorithms.\"\n",
    "4. **Legal Document Processing**: Identify clauses, conditions, or obligations.\n",
    "   - Example: \"The party agrees to pay $1000\" â†’ Chunk: \"pay $1000.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Answer**\n",
    "Semantic chunking is used to group words or phrases based on their **meaning** rather than just their grammatical structure. It is particularly useful for tasks like summarization, topic modeling, and knowledge graph construction. While syntactic chunking focuses on grammar, semantic chunking focuses on **contextual relationships** between words. \n",
    "\n",
    "To summarize:\n",
    "- **Use syntactic chunking** for grammatical analysis.\n",
    "- **Use semantic chunking** for extracting meaningful concepts and relationships.\n",
    "\n",
    "**Boxed Answer:**\n",
    "Semantic chunking groups words/phrases based on meaning, and is ideal for summarization, topic modeling, and knowledge extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Other Types of Chunking Methods**  \n",
    "Chunking is a way of breaking down large text into smaller pieces. Depending on your use case (AI models, search, summarization, etc.), different chunking strategies can be used.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Fixed-Length Chunking**  \n",
    "Splits text into fixed-size chunks (e.g., 1000 characters or 200 tokens).  \n",
    "ðŸ“Œ **Use Case:** When you donâ€™t care about preserving sentence structure, just need equal-sized chunks.  \n",
    "\n",
    "ðŸ”¹ **Example (Character-Based Split)**  \n",
    "```python\n",
    "def fixed_length_chunking(text, chunk_size=1000):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "text = \"This is a sample document. It contains multiple sentences and paragraphs.\"\n",
    "chunks = fixed_length_chunking(text, chunk_size=10)\n",
    "print(chunks)  # ['This is a ', 'sample doc', 'ument. It ', 'contains m', ...]\n",
    "```\n",
    "\n",
    "ðŸ”¹ **Example (Token-Based Split using `tiktoken`)**  \n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "def token_based_chunking(text, chunk_size=200):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "tokens_chunks = token_based_chunking(text, chunk_size=20)\n",
    "print(tokens_chunks)  # Returns chunks of tokenized text\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Sentence-Based Chunking**  \n",
    "Splits text **at sentence boundaries** to avoid cutting sentences in half.  \n",
    "ðŸ“Œ **Use Case:** When meaning should be preserved, useful for AI processing like summarization.  \n",
    "\n",
    "ðŸ”¹ **Example (Using `nltk`)**  \n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_based_chunking(text, max_sentences=2):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [\". \".join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
    "\n",
    "text = \"This is sentence one. This is sentence two. This is sentence three. This is sentence four.\"\n",
    "chunks = sentence_based_chunking(text, max_sentences=2)\n",
    "print(chunks)  # ['This is sentence one. This is sentence two', 'This is sentence three. This is sentence four']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Semantic Chunking (Using Embeddings)**  \n",
    "Uses **semantic meaning** to split text into coherent sections instead of arbitrary cuts.  \n",
    "ðŸ“Œ **Use Case:** When preserving meaning is critical, such as **retrieval-augmented generation (RAG)** for LLMs.  \n",
    "\n",
    "ðŸ”¹ **Example (Using `langchain` Sentence Transformers)**  \n",
    "```python\n",
    "from langchain.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "splitter = SemanticChunker(embedding_model)\n",
    "chunks = splitter.split_text(\"This is a paragraph about AI. AI is transforming industries. Another topic is climate change.\")\n",
    "print(chunks)\n",
    "```\n",
    "ðŸ”¹ **How It Works?**  \n",
    "- Uses **embeddings** (word meaning representations) to group sentences that are semantically related.  \n",
    "- Avoids cutting related content in half.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Overlapping Chunking (Sliding Window)**  \n",
    "Splits text into chunks with **overlapping portions** for better context retention.  \n",
    "ðŸ“Œ **Use Case:** When using AI models where **previous context matters**, like chat history or document search.  \n",
    "\n",
    "ðŸ”¹ **Example (Using `RecursiveCharacterTextSplitter`)**  \n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(\"This is a long document. It contains important information that should not be lost.\")\n",
    "print(chunks)\n",
    "```\n",
    "ðŸ”¹ **How It Works?**  \n",
    "- Each chunk has **some overlap** with the previous chunk (100 characters in this case).  \n",
    "- Ensures **smooth transition** between chunks when feeding them to AI models.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Paragraph-Based Chunking**  \n",
    "Splits text **at paragraph boundaries** instead of arbitrary sizes.  \n",
    "ðŸ“Œ **Use Case:** When working with structured text like **articles, books, or reports**.  \n",
    "\n",
    "ðŸ”¹ **Example (Splitting by Double Newline `\\n\\n`)**  \n",
    "```python\n",
    "def paragraph_chunking(text):\n",
    "    return text.split(\"\\n\\n\")  # Assumes paragraphs are separated by double newlines\n",
    "\n",
    "text = \"Paragraph 1.\\n\\nParagraph 2.\\n\\nParagraph 3.\"\n",
    "chunks = paragraph_chunking(text)\n",
    "print(chunks)  # ['Paragraph 1.', 'Paragraph 2.', 'Paragraph 3.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Topic-Based Chunking (Hierarchical Splitting)**  \n",
    "Splits text based on **topics or section headings**.  \n",
    "ðŸ“Œ **Use Case:** When working with structured documents like **research papers, books, or legal documents**.  \n",
    "\n",
    "ðŸ”¹ **Example (Using Regular Expressions to Detect Headings)**  \n",
    "```python\n",
    "import re\n",
    "\n",
    "def topic_based_chunking(text):\n",
    "    return re.split(r'\\n\\s*\\d+\\.\\s', text)  # Splits at numbered headings like \"1. Introduction\"\n",
    "\n",
    "text = \"1. Introduction\\nThis is the intro.\\n2. Methods\\nThis is the methods section.\"\n",
    "chunks = topic_based_chunking(text)\n",
    "print(chunks)  # ['1. Introduction\\nThis is the intro.', '2. Methods\\nThis is the methods section.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Hybrid Chunking**  \n",
    "Combines **multiple chunking strategies** (e.g., paragraph + token-based).  \n",
    "ðŸ“Œ **Use Case:** When you need a balance between **structure and token limits**.  \n",
    "\n",
    "ðŸ”¹ **Example (First Paragraph, then Token-Based Split)**  \n",
    "```python\n",
    "def hybrid_chunking(text, token_limit=300):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    token_chunks = []\n",
    "    for para in paragraphs:\n",
    "        token_chunks.extend(token_based_chunking(para, chunk_limit=token_limit))\n",
    "    return token_chunks\n",
    "```\n",
    "ðŸ”¹ **Why Use Hybrid?**  \n",
    "- Ensures **natural structure** (paragraphs).  \n",
    "- Respects **token constraints** for AI models.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ”¹ Summary Table**\n",
    "| **Chunking Method** | **Use Case** | **Pros** | **Cons** |\n",
    "|----------------|----------------|----------------|----------------|\n",
    "| **Fixed-Length Chunking** | General purpose, simple splitting | Fast & easy | May break sentences mid-way |\n",
    "| **Sentence-Based Chunking** | Summarization, NLP models | Preserves meaning | Uneven chunk sizes |\n",
    "| **Semantic Chunking** | AI retrieval, LLMs | Smart splits | Requires embeddings model |\n",
    "| **Overlapping Chunking** | LLM context retention | Better AI responses | Higher token cost |\n",
    "| **Paragraph-Based Chunking** | Structured documents (articles, books) | Retains formatting | Large paragraph sizes |\n",
    "| **Topic-Based Chunking** | Research papers, structured reports | Logical splits | Needs clean headings |\n",
    "| **Hybrid Chunking** | AI applications with token limits | Best of both worlds | More complex |\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ’¡ Which One Should You Use?**\n",
    "- **For AI models (like GPT, Llama2)** â†’ **Overlapping, Semantic, or Hybrid Chunking**  \n",
    "- **For Summarization & NLP tasks** â†’ **Sentence-Based or Paragraph-Based Chunking**  \n",
    "- **For Token-Limited Models (OpenAI, Anthropic, etc.)** â†’ **Fixed-Length or Token-Based Chunking**  \n",
    "- **For Legal/Research Docs** â†’ **Topic-Based or Paragraph-Based Chunking**  \n",
    "\n",
    "Would you like a hands-on example for your specific use case? ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
